{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "source": [
    "# Création d'un objet qui *streame* les lignes d'un fichier pour économiser de la RAM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    \"\"\"Tokenize and Lemmatize sentences\"\"\"\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename, encoding='utf-8', errors=\"backslashreplace\"):\n",
    "            yield [unidecode(w.lower()) for w in wordpunct_tokenize(line)]"
   ]
  },
  {
   "source": [
    "# Chargement et traitement des phrases du corpus"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = f\"../data/sents.txt\"\n",
    "sentences = MySentences(infile)"
   ]
  },
  {
   "source": [
    "Les 3 cellules qui suivent servent à montrer le résultat, mais ne les excécutez pas lorsque vous analysez le corpus entier.\n",
    "Car lorsque le volume de texte est grand, il vaut mieux utiliser un générateur (comme MySentences ci-dessus) qui économise la RAM en streamant les phrases depuis le disque dur."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 2min 35s, sys: 3.98 s, total: 2min 39s\nWall time: 2min 42s\n"
     ]
    }
   ],
   "source": [
    "%time sentences = [s for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8695925"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['6',\n",
       " 'et',\n",
       " 'de',\n",
       " 'substituer',\n",
       " 'de',\n",
       " 'nouvelles',\n",
       " 'dispositions',\n",
       " 'a',\n",
       " 'celles',\n",
       " 'de',\n",
       " 'l',\n",
       " \"'\",\n",
       " 'art',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "sentences[123]"
   ]
  },
  {
   "source": [
    "# Détection des bigrams\n",
    "\n",
    "Article intéressant sur le sujet : https://towardsdatascience.com/word2vec-for-phrases-learning-embeddings-for-more-than-one-word-727b6cf723cf"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phrases = Phrases(sentences)"
   ]
  },
  {
   "source": [
    "L'object `phrases` peut être vu comme un large dictionnaire d'expressions multi-mots associées à un score, le *PMI-like scoring*. Ce dictionnaire est construit par un apprentissage sur base d'exemples.\n",
    "Voir les références ci-dessous :\n",
    "- https://arxiv.org/abs/1310.4546\n",
    "- https://en.wikipedia.org/wiki/Pointwise_mutual_information"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "collections.defaultdict"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "type(bigram_phrases.vocab)"
   ]
  },
  {
   "source": [
    "Il contient de nombreuses clés qui sont autant de termes observés dans le corpus"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5529344"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "len(bigram_phrases.vocab.keys())"
   ]
  },
  {
   "source": [
    "Prenons une clé au hasard :"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "b'prise_le'\n"
     ]
    }
   ],
   "source": [
    "key_ = list(bigram_phrases.vocab.keys())[1344]\n",
    "print(key_)"
   ]
  },
  {
   "source": [
    "Le dictionnaire indique le score de cette coocurrence :"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "213"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "bigram_phrases.vocab[key_]"
   ]
  },
  {
   "source": [
    "Lorsque l'instance de `Phrases` a été entraînée, elle peut concaténer les bigrams dans les phrases lorsque c'est pertinent."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 140 µs, sys: 5 µs, total: 145 µs\nWall time: 147 µs\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['la',\n",
       " 'question',\n",
       " 'serait',\n",
       " 'simplifiee',\n",
       " ',',\n",
       " 'si',\n",
       " 'la',\n",
       " 'societe_philanthropique',\n",
       " 'nous',\n",
       " 'indiquait',\n",
       " 'de',\n",
       " 'quelle',\n",
       " 'maniere',\n",
       " 'elle',\n",
       " 'pourvoit',\n",
       " 'aujourd',\n",
       " \"'\",\n",
       " 'hui',\n",
       " 'aux',\n",
       " 'besoins',\n",
       " 'des',\n",
       " 'malheureux',\n",
       " 'qu',\n",
       " \"'\",\n",
       " 'elle',\n",
       " 'entretient',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "%time bigram_phrases[sentences[267]]"
   ]
  },
  {
   "source": [
    "# Conversion des `Phrases` en objet `Phraser`\n",
    "\n",
    "`Phraser` est un alias pour `gensim.models.phrases.FrozenPhrases`, voir ici https://radimrehurek.com/gensim/models/phrases.html.\n",
    "\n",
    "Le `Phraser` est une version *light* du `Phrases`, plus optimale pour transformer les phrases en concaténant les bigrams."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phraser = Phraser(phrases_model=bigram_phrases)"
   ]
  },
  {
   "source": [
    "Le `Phraser` est un objet qui converti certains unigrams d'une liste en bigrams lorsqu'ils ont été identifiés comme pertinents."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 105 µs, sys: 0 ns, total: 105 µs\nWall time: 108 µs\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['38',\n",
       " ',',\n",
       " '875',\n",
       " '1',\n",
       " ',',\n",
       " '875',\n",
       " 'metres',\n",
       " 'de',\n",
       " 'caves',\n",
       " ',',\n",
       " 'a',\n",
       " '5',\n",
       " 'centimes',\n",
       " 'par',\n",
       " 'metre',\n",
       " 'et',\n",
       " 'demi',\n",
       " ',',\n",
       " 'pendant',\n",
       " '311',\n",
       " 'jours',\n",
       " '23',\n",
       " ',',\n",
       " '325',\n",
       " 'total',\n",
       " 'f',\n",
       " 'r',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "%time bigram_phraser[sentences[78]]"
   ]
  },
  {
   "source": [
    "# Extraction des trigrams"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Nous répétons l'opération en envoyant cette fois la liste de bigrams afin d'extraire les trigrams."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_phrases = Phrases(bigram_phraser[sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_phraser = Phraser(phrases_model=trigram_phrases)"
   ]
  },
  {
   "source": [
    "# Création d'un corpus d'unigrams, bigrams, trigrams"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(trigram_phraser[bigram_phraser[sentences]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "Error",
     "evalue": "Pip module Unable to parse debugpy output, please log an issue with https://github.com/microsoft/vscode-jupyter is required for debugging cells. You will need to install it to debug cells.",
     "traceback": [
      "Error: Pip module Unable to parse debugpy output, please log an issue with https://github.com/microsoft/vscode-jupyter is required for debugging cells. You will need to install it to debug cells.",
      "at b.parseConnectInfo (/Users/charalambos/.vscode/extensions/ms-toolsai.jupyter-2020.11.372831992/out/client/extension.js:49:490044)",
      "at b.connectToLocal (/Users/charalambos/.vscode/extensions/ms-toolsai.jupyter-2020.11.372831992/out/client/extension.js:49:490571)",
      "at async b.connect (/Users/charalambos/.vscode/extensions/ms-toolsai.jupyter-2020.11.372831992/out/client/extension.js:49:488448)",
      "at async b.startDebugSession (/Users/charalambos/.vscode/extensions/ms-toolsai.jupyter-2020.11.372831992/out/client/extension.js:49:487595)",
      "at async f.submitCode (/Users/charalambos/.vscode/extensions/ms-toolsai.jupyter-2020.11.372831992/out/client/extension.js:32:565062)",
      "at async f.handleRunByLine (/Users/charalambos/.vscode/extensions/ms-toolsai.jupyter-2020.11.372831992/out/client/extension.js:9:173151)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Word2Vec(\n",
    "    corpus, # On passe le corpus de ngrams que nous venons de créer\n",
    "    size=64, # Le nombre de dimensions dans lesquelles le contexte des mots devra être réduit, aka. vector_size\n",
    "    window=5, # La taille du \"contexte\", ici 5 mots avant et après le mot observé\n",
    "    min_count=5, # On ignore les mots qui n'apparaissent pas au moins 5 fois dans le corpus\n",
    "    workers=4, # Permet de paralléliser l'entraînement du modèle en 4 threads\n",
    "    iter=8 # Nombre d'itérations du réseau de neurones sur le jeu de données pour ajuster les paramètres avec la descende de gradient, aka. epochs.\n",
    ")"
   ]
  },
  {
   "source": [
    "## Remarque\n",
    "\n",
    "Vous voyez ici que l'entrainement du modèle est parallélisé (sur 4 workers).\n",
    "\n",
    "Lors qu'on parallélise l'entrainement du modèle, 4 modèles \"séparés\" sont entrainés sur environ un quart des phrases.\n",
    "\n",
    "Ensuite, les résultats sont agrégés pour ne plus faire qu'un seul modèle.\n",
    "\n",
    "On ne peut prédire quel worker aura quelle phrase, car il y a des aléas lors de la parallélisation (p. ex. un worker qui serait plus lent, etc.).\n",
    "\n",
    "Du coup, les valeurs peuvent varier légèrement d'un entrainement à l'autre.\n",
    "\n",
    "Mais, globalement, les résultats restent cohérents."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = f\"../data/bulletins.model\"\n",
    "model.save(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}