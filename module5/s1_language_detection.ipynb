{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Detect languages used in bulletins"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import langid\n",
    "import pycountry"
   ]
  },
  {
   "source": [
    "Le code ci-dessous permet de forcer l'algorithme à choisir entre le français ou le néerlandais"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "langid.set_languages(['fr', 'nl'])"
   ]
  },
  {
   "source": [
    "Nous listons tous les fichiers .txt dans le corpus"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2828 TXT files found\n"
     ]
    }
   ],
   "source": [
    "root = \"../data/txt/\"\n",
    "txts = os.listdir(root)\n",
    "print(f\"{len(txts)} TXT files found\")"
   ]
  },
  {
   "source": [
    "Nous allons lire chaque fichier, détecter la langue, et incrémenter `lang_dict`lorsqu'une langue est détectée.\n",
    "\n",
    "**Important** : pour détecter les langues sur tous les documents, mettez `limit = None` ci-dessous."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit = 500\n",
    "limit = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_dict = defaultdict(int)\n",
    "txts = txts[:limit] if limit else txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 document(s) processed...\n",
      "50 document(s) processed...\n",
      "100 document(s) processed...\n",
      "150 document(s) processed...\n",
      "200 document(s) processed...\n",
      "Bxl_1869_Tome_I1_Part_4.txt contains only 4 characters, treating as unknown\n",
      "250 document(s) processed...\n",
      "300 document(s) processed...\n",
      "350 document(s) processed...\n",
      "400 document(s) processed...\n",
      "450 document(s) processed...\n",
      "500 document(s) processed...\n",
      "550 document(s) processed...\n",
      "600 document(s) processed...\n",
      "650 document(s) processed...\n",
      "700 document(s) processed...\n",
      "750 document(s) processed...\n",
      "800 document(s) processed...\n",
      "850 document(s) processed...\n",
      "900 document(s) processed...\n",
      "950 document(s) processed...\n",
      "1000 document(s) processed...\n",
      "1050 document(s) processed...\n",
      "1100 document(s) processed...\n",
      "1150 document(s) processed...\n",
      "Bxl_1925_Tome_II1_2_Part_8.txt contains only 9 characters, treating as unknown\n",
      "1200 document(s) processed...\n",
      "1250 document(s) processed...\n",
      "Bxl_1929_Tome_I_Part_10.txt contains only 1 characters, treating as unknown\n",
      "1300 document(s) processed...\n",
      "1350 document(s) processed...\n",
      "1400 document(s) processed...\n",
      "1450 document(s) processed...\n",
      "Bxl_1946_Tome_II_Part_14.txt contains only 1 characters, treating as unknown\n",
      "1500 document(s) processed...\n",
      "1550 document(s) processed...\n",
      "1600 document(s) processed...\n",
      "Bxl_1952_Tome_I_Part_9.txt contains only 2 characters, treating as unknown\n",
      "1650 document(s) processed...\n",
      "1700 document(s) processed...\n",
      "1750 document(s) processed...\n",
      "Bxl_1957_Tome_II2_Part_10.txt contains only 9 characters, treating as unknown\n",
      "1800 document(s) processed...\n",
      "Bxl_1957_Tome_I_Part_12.txt contains only 2 characters, treating as unknown\n",
      "Bxl_1958_Tome_RptAn_Part_10.txt contains only 9 characters, treating as unknown\n",
      "1850 document(s) processed...\n",
      "1900 document(s) processed...\n",
      "1950 document(s) processed...\n",
      "2000 document(s) processed...\n",
      "2050 document(s) processed...\n",
      "2100 document(s) processed...\n",
      "2150 document(s) processed...\n",
      "Bxl_1967_Tome_I1_Part_11.txt contains only 1 characters, treating as unknown\n",
      "2200 document(s) processed...\n",
      "2250 document(s) processed...\n",
      "Bxl_1969_Tome_II2_Part_14.txt contains only 8 characters, treating as unknown\n",
      "2300 document(s) processed...\n",
      "Bxl_1970_Tome_II1_Part_11.txt contains only 3 characters, treating as unknown\n",
      "2350 document(s) processed...\n",
      "2400 document(s) processed...\n",
      "2450 document(s) processed...\n",
      "2500 document(s) processed...\n",
      "2550 document(s) processed...\n",
      "Bxl_1976_Tome_I2_Part_7.txt contains only 11 characters, treating as unknown\n",
      "2600 document(s) processed...\n",
      "2650 document(s) processed...\n",
      "2700 document(s) processed...\n",
      "2750 document(s) processed...\n",
      "2800 document(s) processed...\n"
     ]
    }
   ],
   "source": [
    "for i, txt in enumerate(sorted(txts)):\n",
    "    if i % 50 == 0:\n",
    "        print(f'{i} document(s) processed...')\n",
    "    text = open(os.path.join(root, txt), encoding=\"ISO-8859-1\").read()\n",
    "    text_length = len(text)\n",
    "    if text_length > 20:\n",
    "        lang, conf = langid.classify(text)\n",
    "        lang_dict[lang] += 1\n",
    "    else:\n",
    "        print(f\"{txt} contains only {text_length} characters, treating as unknown\")\n",
    "        lang_dict['n/a'] += 1"
   ]
  },
  {
   "source": [
    "Nous allons trier les langues de la plus fréquente à la plus rare."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "sorted_lang = sorted(lang_dict.items(), key=lambda kv: kv[1], reverse=True)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 12,
   "outputs": []
  },
  {
   "source": [
    "Et nous imprimons les résultats..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "for lang_code, nb_docs in sorted_lang:\n",
    "    language = pycountry.languages.get(alpha_2=lang_code)\n",
    "    try:\n",
    "        lang_name = language.name\n",
    "    except AttributeError:\n",
    "        lang_name = language\n",
    "    print(f\"{lang_name}\\t{nb_docs}\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "French\t2725\nDutch\t91\nNone\t12\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}